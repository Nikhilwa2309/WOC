# -*- coding: utf-8 -*-
"""WOC PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCu4_3i6f0z5hIM05bl71UYRME7f6-WF
"""



"""Linear Regression

"""



"""y=wx+c
y--> Dependent Variable
x--> Independent variable
w-->weight
b-->bias


GRADIENT DESCENT
w=w-a*dw
b=b-a*db

# BUILDING POLYNOMIAL REGRESSION FROM SCRATCH
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the dataset
file_path = '/linear_regression_train.csv'
data = pd.read_csv(file_path)

# Step 2: Preprocess the data
# Replace NaN and infinite values
data.replace([np.inf, -np.inf], np.nan, inplace=True)
data.fillna(0, inplace=True)

# Separate features (X) and target (y)
X = data.drop(columns=['Target', 'ID'])  # Drop target and ID columns
y = data['Target']

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Apply Polynomial Regression
# Create polynomial features
degree = 2  # Adjust the degree as needed
poly = PolynomialFeatures(degree=degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Step 5: Train the model
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Step 6: Evaluate the model
y_pred = model.predict(X_test_poly)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Output results
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

"""# EVALUATION ON HIDDEN NEW DATA SET"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

# Step 1: Load the new dataset (hidden dataset)
new_file_path = '/linear_regression_test.csv'  # Replace with your new dataset file path
new_data = pd.read_csv(new_file_path)

# Step 2: Check the columns in the new dataset
print(new_data.columns)  # Check column names to ensure correct reference

# Step 3: Preprocess the new dataset
# Replace NaN and infinite values (same as during training)
new_data.replace([np.inf, -np.inf], np.nan, inplace=True)
new_data.fillna(0, inplace=True)

# Step 4: Separate features (X) for the new dataset
X_new = new_data.drop(columns=['ID'])  # Drop 'ID' column (no target column available)

# Step 5: Apply the same polynomial feature transformation
degree = 2  # Ensure this matches the degree used during training
poly = PolynomialFeatures(degree=degree)

# Transform the features into polynomial features (same transformation as training data)
X_new_poly = poly.fit_transform(X_new)

# Step 6: Use the trained model to make predictions
# Assuming the model is already trained and available as 'model'
y_new_pred = model.predict(X_new_poly)
  # Replace 'ActualTarget' with the actual column name in your dataset


# Output the predictions
print("Predictions on the new dataset:", y_new_pred)

# Optionally, visualize the predictions (if you know the target values, you can plot actual vs predicted)
plt.plot(X_new, y_new_pred, color='blue', label='Polynomial fit')
plt.title('Polynomial Regression: Predictions on New Data')
plt.xlabel('Features')
plt.ylabel('Predicted Values')
plt.legend()
plt.show()

"""#LINEAR REGRESSION FROM SCRATCH"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Load the dataset
file_path = '/Salary_Data.csv'  # Replace with your dataset file path
data = pd.read_csv(file_path)

# Step 2: Preprocess the data
# Assuming the target column is 'Target' and features are all other columns
X = data.drop(columns=['Salary'])
y = data['Salary']

# Step 3: Feature Scaling (Standardization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 4: Add a column of ones for the bias term (intercept)
X_scaled = np.c_[np.ones(X_scaled.shape[0]), X_scaled]

# Step 5: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Step 6: Initialize parameters (theta) for Linear Regression
theta = np.zeros(X_train.shape[1])  # Initialize with zeros (one value per feature + bias term)

# Step 7: Define the hypothesis function (h(X) = X * theta)
def hypothesis(X, theta):
    return np.dot(X, theta)

# Step 8: Define the cost function (Mean Squared Error)
def cost_function(X, y, theta):
    m = len(y)
    return (1 / (2 * m)) * np.sum((hypothesis(X, theta) - y) ** 2)

# Step 9: Implement gradient descent
def gradient_descent(X, y, theta, learning_rate=0.01, iterations=1000):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        # Compute the gradient (derivative of the cost function w.r.t. theta)
        gradients = (1 / m) * np.dot(X.T, hypothesis(X, theta) - y)
        # Update theta using the gradients
        theta -= learning_rate * gradients
        # Record the cost for each iteration
        cost_history.append(cost_function(X, y, theta))

    return theta, cost_history

# Step 10: Train the model using gradient descent
theta_optimal, cost_history = gradient_descent(X_train, y_train, theta, learning_rate=0.01, iterations=1000)

# Step 11: Make predictions on the test set
y_pred = hypothesis(X_test, theta_optimal)

# Step 12: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Output results
print(f"Optimal Theta: {theta_optimal}")
print(f"Mean Squared Error: {mse}")
print(f"RÂ² Score: {r2}")

# Step 13: Show the predicted values and the actual values for comparison
print("\nPredicted values:")
print(y_pred)

print("\nActual values:")
print(y_test.values)

# Step 14: Visualization - Plot the predicted vs actual values
plt.figure(figsize=(10, 6))

# Plotting actual vs predicted values
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Fit')
plt.title('Predicted vs Actual Values')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.grid(True)
plt.show()

"""#Logistic regression"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
file_path = '/content/binary_classification_train.csv'
data = pd.read_csv(file_path)

# Preprocessing
X = data.iloc[:, 1:-1].values  # Features (excluding ID and Class)
y = data['Class'].values       # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression Implementation
class LogisticRegression:
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.iterations):
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)

            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # Update weights and bias
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self.sigmoid(linear_model)
        return np.array([1 if i > 0.5 else 0 for i in y_predicted])

# Train the model
model = LogisticRegression(learning_rate=0.01, iterations=1000)
model.fit(X_train, y_train)

# Test the model
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

"""#TESTING FROM THE HIDDEN DATA SET"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

file_path = '/content/binary_classification_train.csv'
n_data = pd.read_csv(file_path)

# Remove the 18th column (index 17)
n_data = n_data.drop(n_data.columns[17], axis=1)

X_test = n_data.iloc[:, 1:].values

y_pred = model.predict(X_test)

print("Predicted labels:", y_pred[:10])

import matplotlib.pyplot as plt

# Calculate accuracy for training data
y_train_pred = model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)

# Plotting accuracy
datasets = ['Training', 'Testing']
accuracies = [train_accuracy * 100, accuracy * 100]

plt.figure(figsize=(8, 5))
plt.bar(datasets, accuracies, color=['blue', 'orange'])
plt.ylim(0, 100)  # Accuracy is a percentage
plt.title('Model Accuracy')
plt.ylabel('Accuracy (%)')
plt.xlabel('Dataset')
plt.text(0, train_accuracy * 100 + 1, f"{train_accuracy * 100:.2f}%", ha='center', color='blue')
plt.text(1, accuracy * 100 + 1, f"{accuracy * 100:.2f}%", ha='center', color='orange')
plt.show()

"""# MULTI CLASSIFICATION DATA"""

# Importing necessary libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score, classification_report

# Loading a dataset (Iris dataset as an example)
file_path = '/content/multi_classification_train.csv'
m_data = pd.read_csv(file_path)

X = m_data  # Features
y = m_data.Class  # Labels

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features (optional but recommended for most models)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initializing and training the Logistic Regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Printing classification report for detailed performance metrics
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Loading the original dataset (Iris dataset as an example)
file_path = '/content/multi_classification_train.csv'
m_data = pd.read_csv(file_path)
# Assuming new_data is your DataFrame
m_data = m_data.drop(m_data.columns[-2], axis=1)




X = m_data  # Features
y = m_data.Class  # Labels
# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initializing and training the Logistic Regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Saving the trained model and scaler (for future use)
import joblib
joblib.dump(model, 'logistic_regression_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Load the hidden new dataset (e.g., 'new_data.csv')
# Assuming the new dataset has the same feature columns as the original dataset
new_data = pd.read_csv('/content/multi_classification_test.csv')  # Replace with your dataset path
new_features = new_data.values  # Convert the dataset to a NumPy array

# Preprocessing the new dataset (standardizing)
scaler = joblib.load('scaler.pkl')
new_features_scaled = scaler.transform(new_features)

# Load the trained model
model = joblib.load('logistic_regression_model.pkl')

# Predicting using the trained model
predictions = model.predict(new_features_scaled)

# Printing the predictions
print("Predictions for the new dataset:")
print(predictions)
print(f"Accuracy on the new dataset: {accuracy * 100:.2f}%")

# If you have true labels for the new dataset, you can also evaluate the performance
# Example (if you have true labels `true_labels` for the new data):
# accuracy = accuracy_score(true_labels, predictions)
# print(f"Accuracy on the new dataset: {accuracy * 100:.2f}%")

